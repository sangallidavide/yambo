<center><a href="mpits.c">Actual source code: mpits.c</a></center><br>

<html>
<head> <link rel="canonical" href="http://www.mcs.anl.gov/petsc/petsc-current/src/sys/utils/mpits.c.html" />
<title></title>
<meta name="generator" content="c2html 0.9.4">
<meta name="date" content="2016-10-03T02:15:18+00:00">
</head>

<body bgcolor="#FFFFFF">
   <div id="version" align=right><b>petsc-3.7.4 2016-10-02</b></div>
   <div id="bugreport" align=right><a href="mailto:petsc-maint@mcs.anl.gov?subject=Typo or Error in Documentation &body=Please describe the typo or error in the documentation: petsc-3.7.4 v3.7.4 src/sys/utils/mpits.c.html "><small>Report Typos and Errors</small></a></div>
<pre width="80"><a name="line1">  1: </a><font color="#A020F0">#include &lt;petscsys.h&gt;        </font><font color="#B22222">/*I  "petscsys.h"  I*/</font><font color="#A020F0"></font>

<a name="line3">  3: </a><a href="../../../docs/manualpages/Profiling/PetscLogEvent.html#PetscLogEvent">PetscLogEvent</a> PETSC_BuildTwoSided,PETSC_BuildTwoSidedF;

<a name="line5">  5: </a>const char *const PetscBuildTwoSidedTypes[] = {
<a name="line6">  6: </a>  <font color="#666666">"ALLREDUCE"</font>,
<a name="line7">  7: </a>  <font color="#666666">"IBARRIER"</font>,
<a name="line8">  8: </a>  <font color="#666666">"REDSCATTER"</font>,
<a name="line9">  9: </a>  <font color="#666666">"<a href="../../../docs/manualpages/Sys/PetscBuildTwoSidedType.html#PetscBuildTwoSidedType">PetscBuildTwoSidedType</a>"</font>,
<a name="line10"> 10: </a>  <font color="#666666">"PETSC_BUILDTWOSIDED_"</font>,
<a name="line11"> 11: </a>  0
<a name="line12"> 12: </a>};

<a name="line14"> 14: </a>static <a href="../../../docs/manualpages/Sys/PetscBuildTwoSidedType.html#PetscBuildTwoSidedType">PetscBuildTwoSidedType</a> _twosided_type = PETSC_BUILDTWOSIDED_NOTSET;

<a name="line18"> 18: </a><font color="#B22222">/*@</font>
<a name="line19"> 19: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedSetType.html#PetscCommBuildTwoSidedSetType">PetscCommBuildTwoSidedSetType</a> - set algorithm to use when building two-sided communication</font>

<a name="line21"> 21: </a><font color="#B22222">   Logically Collective</font>

<a name="line23"> 23: </a><font color="#B22222">   Input Arguments:</font>
<a name="line24"> 24: </a><font color="#B22222">+  comm - <a href="../../../docs/manualpages/Sys/PETSC_COMM_WORLD.html#PETSC_COMM_WORLD">PETSC_COMM_WORLD</a></font>
<a name="line25"> 25: </a><font color="#B22222">-  twosided - algorithm to use in subsequent calls to <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>()</font>

<a name="line27"> 27: </a><font color="#B22222">   Level: developer</font>

<a name="line29"> 29: </a><font color="#B22222">   Note:</font>
<a name="line30"> 30: </a><font color="#B22222">   This option is currently global, but could be made per-communicator.</font>

<a name="line32"> 32: </a><font color="#B22222">.seealso: <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>(), <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedGetType.html#PetscCommBuildTwoSidedGetType">PetscCommBuildTwoSidedGetType</a>()</font>
<a name="line33"> 33: </a><font color="#B22222">@*/</font>
<a name="line34"> 34: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSidedSetType"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedSetType.html#PetscCommBuildTwoSidedSetType">PetscCommBuildTwoSidedSetType</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscBuildTwoSidedType.html#PetscBuildTwoSidedType">PetscBuildTwoSidedType</a> twosided)</font></strong>
<a name="line35"> 35: </a>{
<a name="line37"> 37: </a><font color="#A020F0">#if defined(PETSC_USE_DEBUG)</font>
<a name="line39"> 39: </a>    <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> ierr;
<a name="line40"> 40: </a>    <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> b1[2],b2[2];
<a name="line41"> 41: </a>    b1[0] = -(<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>)twosided;
<a name="line42"> 42: </a>    b1[1] = (<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>)twosided;
<a name="line43"> 43: </a>    MPIU_Allreduce(b1,b2,2,MPI_INT,MPI_MAX,comm);
<a name="line44"> 44: </a>    <font color="#4169E1">if</font> (-b2[0] != b2[1]) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(comm,PETSC_ERR_ARG_WRONG,<font color="#666666">"Enum value must be same on all processes"</font>);
<a name="line45"> 45: </a>  }
<a name="line46"> 46: </a><font color="#A020F0">#endif</font>
<a name="line47"> 47: </a>  _twosided_type = twosided;
<a name="line48"> 48: </a>  <font color="#4169E1">return</font>(0);
<a name="line49"> 49: </a>}

<a name="line53"> 53: </a><font color="#B22222">/*@</font>
<a name="line54"> 54: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedGetType.html#PetscCommBuildTwoSidedGetType">PetscCommBuildTwoSidedGetType</a> - set algorithm to use when building two-sided communication</font>

<a name="line56"> 56: </a><font color="#B22222">   Logically Collective</font>

<a name="line58"> 58: </a><font color="#B22222">   Output Arguments:</font>
<a name="line59"> 59: </a><font color="#B22222">+  comm - communicator on which to query algorithm</font>
<a name="line60"> 60: </a><font color="#B22222">-  twosided - algorithm to use for <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>()</font>

<a name="line62"> 62: </a><font color="#B22222">   Level: developer</font>

<a name="line64"> 64: </a><font color="#B22222">.seealso: <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>(), <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedSetType.html#PetscCommBuildTwoSidedSetType">PetscCommBuildTwoSidedSetType</a>()</font>
<a name="line65"> 65: </a><font color="#B22222">@*/</font>
<a name="line66"> 66: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSidedGetType"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedGetType.html#PetscCommBuildTwoSidedGetType">PetscCommBuildTwoSidedGetType</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscBuildTwoSidedType.html#PetscBuildTwoSidedType">PetscBuildTwoSidedType</a> *twosided)</font></strong>
<a name="line67"> 67: </a>{

<a name="line71"> 71: </a>  *twosided = PETSC_BUILDTWOSIDED_NOTSET;
<a name="line72"> 72: </a>  <font color="#4169E1">if</font> (_twosided_type == PETSC_BUILDTWOSIDED_NOTSET) {
<a name="line73"> 73: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_IBARRIER)</font>
<a name="line74"> 74: </a><font color="#A020F0">#  if defined(PETSC_HAVE_MPICH_CH3_SOCK) &amp;&amp; !defined(PETSC_HAVE_MPICH_CH3_SOCK_FIXED_NBC_PROGRESS)</font>
<a name="line75"> 75: </a>    <font color="#B22222">/* Deadlock in Ibarrier: http://trac.mpich.org/projects/mpich/ticket/1785 */</font>
<a name="line76"> 76: </a>    _twosided_type = PETSC_BUILDTWOSIDED_ALLREDUCE;
<a name="line77"> 77: </a><font color="#A020F0">#  else</font>
<a name="line78"> 78: </a>    _twosided_type = PETSC_BUILDTWOSIDED_IBARRIER;
<a name="line79"> 79: </a><font color="#A020F0">#  endif</font>
<a name="line80"> 80: </a><font color="#A020F0">#else</font>
<a name="line81"> 81: </a>    _twosided_type = PETSC_BUILDTWOSIDED_ALLREDUCE;
<a name="line82"> 82: </a><font color="#A020F0">#endif</font>
<a name="line83"> 83: </a>    <a href="../../../docs/manualpages/Sys/PetscOptionsGetEnum.html#PetscOptionsGetEnum">PetscOptionsGetEnum</a>(NULL,NULL,<font color="#666666">"-build_twosided"</font>,PetscBuildTwoSidedTypes,(<a href="../../../docs/manualpages/Sys/PetscEnum.html#PetscEnum">PetscEnum</a>*)&amp;_twosided_type,NULL);
<a name="line84"> 84: </a>  }
<a name="line85"> 85: </a>  *twosided = _twosided_type;
<a name="line86"> 86: </a>  <font color="#4169E1">return</font>(0);
<a name="line87"> 87: </a>}

<a name="line89"> 89: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_IBARRIER) || defined(PETSC_HAVE_MPIX_IBARRIER)</font>

<a name="line93"> 93: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSided_Ibarrier"></a>static <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> PetscCommBuildTwoSided_Ibarrier(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata)</font></strong>
<a name="line94"> 94: </a>{
<a name="line96"> 96: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>    nrecvs,tag,done,i;
<a name="line97"> 97: </a>  MPI_Aint       lb,unitbytes;
<a name="line98"> 98: </a>  char           *tdata;
<a name="line99"> 99: </a>  MPI_Request    *sendreqs,barrier;
<a name="line100">100: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBuffer.html#PetscSegBuffer">PetscSegBuffer</a> segrank,segdata;
<a name="line101">101: </a>  <a href="../../../docs/manualpages/Sys/PetscBool.html#PetscBool">PetscBool</a>      barrier_started;

<a name="line104">104: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDuplicate.html#PetscCommDuplicate">PetscCommDuplicate</a>(comm,&amp;comm,&amp;tag);
<a name="line105">105: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Type_get_extent.html#MPI_Type_get_extent">MPI_Type_get_extent</a>(dtype,&amp;lb,&amp;unitbytes);
<a name="line106">106: </a>  <font color="#4169E1">if</font> (lb != 0) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(comm,PETSC_ERR_SUP,<font color="#666666">"Datatype with nonzero lower bound %ld\n"</font>,(long)lb);
<a name="line107">107: </a>  tdata = (char*)todata;
<a name="line108">108: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(nto,&amp;sendreqs);
<a name="line109">109: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) {
<a name="line110">110: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Issend.html#MPI_Issend">MPI_Issend</a>((void*)(tdata+count*unitbytes*i),count,dtype,toranks[i],tag,comm,sendreqs+i);
<a name="line111">111: </a>  }
<a name="line112">112: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferCreate.html#PetscSegBufferCreate">PetscSegBufferCreate</a>(<font color="#4169E1">sizeof</font>(<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>),4,&amp;segrank);
<a name="line113">113: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferCreate.html#PetscSegBufferCreate">PetscSegBufferCreate</a>(unitbytes,4*count,&amp;segdata);

<a name="line115">115: </a>  nrecvs  = 0;
<a name="line116">116: </a>  barrier = MPI_REQUEST_NULL;
<a name="line117">117: </a>  <font color="#B22222">/* MPICH-3.2 sometimes does not create a request in some "optimized" cases.  This is arguably a standard violation,</font>
<a name="line118">118: </a><font color="#B22222">   * but we need to work around it. */</font>
<a name="line119">119: </a>  barrier_started = <a href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</a>;
<a name="line120">120: </a>  <font color="#4169E1">for</font> (done=0; !done; ) {
<a name="line121">121: </a>    <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> flag;
<a name="line122">122: </a>    MPI_Status  status;
<a name="line123">123: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Iprobe.html#MPI_Iprobe">MPI_Iprobe</a>(MPI_ANY_SOURCE,tag,comm,&amp;flag,&amp;status);
<a name="line124">124: </a>    <font color="#4169E1">if</font> (flag) {                 <font color="#B22222">/* incoming message */</font>
<a name="line125">125: </a>      <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *recvrank;
<a name="line126">126: </a>      void        *buf;
<a name="line127">127: </a>      <a href="../../../docs/manualpages/Sys/PetscSegBufferGet.html#PetscSegBufferGet">PetscSegBufferGet</a>(segrank,1,&amp;recvrank);
<a name="line128">128: </a>      <a href="../../../docs/manualpages/Sys/PetscSegBufferGet.html#PetscSegBufferGet">PetscSegBufferGet</a>(segdata,count,&amp;buf);
<a name="line129">129: </a>      *recvrank = status.MPI_SOURCE;
<a name="line130">130: </a>      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</a>(buf,count,dtype,status.MPI_SOURCE,tag,comm,MPI_STATUS_IGNORE);
<a name="line131">131: </a>      nrecvs++;
<a name="line132">132: </a>    }
<a name="line133">133: </a>    <font color="#4169E1">if</font> (!barrier_started) {
<a name="line134">134: </a>      <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> sent,nsends;
<a name="line135">135: </a>      <a href="../../../docs/manualpages/Sys/PetscMPIIntCast.html#PetscMPIIntCast">PetscMPIIntCast</a>(nto,&amp;nsends);
<a name="line136">136: </a>      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Testall.html#MPI_Testall">MPI_Testall</a>(nsends,sendreqs,&amp;sent,MPI_STATUSES_IGNORE);
<a name="line137">137: </a>      <font color="#4169E1">if</font> (sent) {
<a name="line138">138: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_IBARRIER)</font>
<a name="line139">139: </a>        <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html#MPI_Ibarrier">MPI_Ibarrier</a>(comm,&amp;barrier);
<a name="line140">140: </a><font color="#A020F0">#elif defined(PETSC_HAVE_MPIX_IBARRIER)</font>
<a name="line141">141: </a>        MPIX_Ibarrier(comm,&amp;barrier);
<a name="line142">142: </a><font color="#A020F0">#endif</font>
<a name="line143">143: </a>        barrier_started = <a href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</a>;
<a name="line144">144: </a>        <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(sendreqs);
<a name="line145">145: </a>      }
<a name="line146">146: </a>    } <font color="#4169E1">else</font> {
<a name="line147">147: </a>      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Test.html#MPI_Test">MPI_Test</a>(&amp;barrier,&amp;done,MPI_STATUS_IGNORE);
<a name="line148">148: </a>    }
<a name="line149">149: </a>  }
<a name="line150">150: </a>  *nfrom = nrecvs;
<a name="line151">151: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferExtractAlloc.html#PetscSegBufferExtractAlloc">PetscSegBufferExtractAlloc</a>(segrank,fromranks);
<a name="line152">152: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferDestroy.html#PetscSegBufferDestroy">PetscSegBufferDestroy</a>(&amp;segrank);
<a name="line153">153: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferExtractAlloc.html#PetscSegBufferExtractAlloc">PetscSegBufferExtractAlloc</a>(segdata,fromdata);
<a name="line154">154: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferDestroy.html#PetscSegBufferDestroy">PetscSegBufferDestroy</a>(&amp;segdata);
<a name="line155">155: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDestroy.html#PetscCommDestroy">PetscCommDestroy</a>(&amp;comm);
<a name="line156">156: </a>  <font color="#4169E1">return</font>(0);
<a name="line157">157: </a>}
<a name="line158">158: </a><font color="#A020F0">#endif</font>

<a name="line162">162: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSided_Allreduce"></a>static <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> PetscCommBuildTwoSided_Allreduce(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata)</font></strong>
<a name="line163">163: </a>{
<a name="line165">165: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>    size,*iflags,nrecvs,tag,*franks,i;
<a name="line166">166: </a>  MPI_Aint       lb,unitbytes;
<a name="line167">167: </a>  char           *tdata,*fdata;
<a name="line168">168: </a>  MPI_Request    *reqs,*sendreqs;
<a name="line169">169: </a>  MPI_Status     *statuses;

<a name="line172">172: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>(comm,&amp;size);
<a name="line173">173: </a>  <a href="../../../docs/manualpages/Sys/PetscCalloc1.html#PetscCalloc1">PetscCalloc1</a>(size,&amp;iflags);
<a name="line174">174: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) iflags[toranks[i]] = 1;
<a name="line175">175: </a>  <a href="../../../docs/manualpages/Sys/PetscGatherNumberOfMessages.html#PetscGatherNumberOfMessages">PetscGatherNumberOfMessages</a>(comm,iflags,NULL,&amp;nrecvs);
<a name="line176">176: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(iflags);

<a name="line178">178: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDuplicate.html#PetscCommDuplicate">PetscCommDuplicate</a>(comm,&amp;comm,&amp;tag);
<a name="line179">179: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Type_get_extent.html#MPI_Type_get_extent">MPI_Type_get_extent</a>(dtype,&amp;lb,&amp;unitbytes);
<a name="line180">180: </a>  <font color="#4169E1">if</font> (lb != 0) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(comm,PETSC_ERR_SUP,<font color="#666666">"Datatype with nonzero lower bound %ld\n"</font>,(long)lb);
<a name="line181">181: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</a>(nrecvs*count*unitbytes,&amp;fdata);
<a name="line182">182: </a>  tdata    = (char*)todata;
<a name="line183">183: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc2.html#PetscMalloc2">PetscMalloc2</a>(nto+nrecvs,&amp;reqs,nto+nrecvs,&amp;statuses);
<a name="line184">184: </a>  sendreqs = reqs + nrecvs;
<a name="line185">185: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nrecvs; i++) {
<a name="line186">186: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</a>((void*)(fdata+count*unitbytes*i),count,dtype,MPI_ANY_SOURCE,tag,comm,reqs+i);
<a name="line187">187: </a>  }
<a name="line188">188: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) {
<a name="line189">189: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Isend.html#MPI_Isend">MPI_Isend</a>((void*)(tdata+count*unitbytes*i),count,dtype,toranks[i],tag,comm,sendreqs+i);
<a name="line190">190: </a>  }
<a name="line191">191: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</a>(nto+nrecvs,reqs,statuses);
<a name="line192">192: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(nrecvs,&amp;franks);
<a name="line193">193: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nrecvs; i++) franks[i] = statuses[i].MPI_SOURCE;
<a name="line194">194: </a>  <a href="../../../docs/manualpages/Sys/PetscFree2.html#PetscFree2">PetscFree2</a>(reqs,statuses);
<a name="line195">195: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDestroy.html#PetscCommDestroy">PetscCommDestroy</a>(&amp;comm);

<a name="line197">197: </a>  *nfrom            = nrecvs;
<a name="line198">198: </a>  *fromranks        = franks;
<a name="line199">199: </a>  *(void**)fromdata = fdata;
<a name="line200">200: </a>  <font color="#4169E1">return</font>(0);
<a name="line201">201: </a>}

<a name="line203">203: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_REDUCE_SCATTER_BLOCK)</font>
<a name="line206">206: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSided_RedScatter"></a>static <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> PetscCommBuildTwoSided_RedScatter(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata)</font></strong>
<a name="line207">207: </a>{
<a name="line209">209: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>    size,*iflags,nrecvs,tag,*franks,i;
<a name="line210">210: </a>  MPI_Aint       lb,unitbytes;
<a name="line211">211: </a>  char           *tdata,*fdata;
<a name="line212">212: </a>  MPI_Request    *reqs,*sendreqs;
<a name="line213">213: </a>  MPI_Status     *statuses;

<a name="line216">216: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>(comm,&amp;size);
<a name="line217">217: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(size,&amp;iflags);
<a name="line218">218: </a>  <a href="../../../docs/manualpages/Sys/PetscMemzero.html#PetscMemzero">PetscMemzero</a>(iflags,size*<font color="#4169E1">sizeof</font>(*iflags));
<a name="line219">219: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) iflags[toranks[i]] = 1;
<a name="line220">220: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Reduce_scatter_block.html#MPI_Reduce_scatter_block">MPI_Reduce_scatter_block</a>(iflags,&amp;nrecvs,1,MPI_INT,MPI_SUM,comm);
<a name="line221">221: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(iflags);

<a name="line223">223: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDuplicate.html#PetscCommDuplicate">PetscCommDuplicate</a>(comm,&amp;comm,&amp;tag);
<a name="line224">224: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Type_get_extent.html#MPI_Type_get_extent">MPI_Type_get_extent</a>(dtype,&amp;lb,&amp;unitbytes);
<a name="line225">225: </a>  <font color="#4169E1">if</font> (lb != 0) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(comm,PETSC_ERR_SUP,<font color="#666666">"Datatype with nonzero lower bound %ld\n"</font>,(long)lb);
<a name="line226">226: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc.html#PetscMalloc">PetscMalloc</a>(nrecvs*count*unitbytes,&amp;fdata);
<a name="line227">227: </a>  tdata    = (char*)todata;
<a name="line228">228: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc2.html#PetscMalloc2">PetscMalloc2</a>(nto+nrecvs,&amp;reqs,nto+nrecvs,&amp;statuses);
<a name="line229">229: </a>  sendreqs = reqs + nrecvs;
<a name="line230">230: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nrecvs; i++) {
<a name="line231">231: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Irecv.html#MPI_Irecv">MPI_Irecv</a>((void*)(fdata+count*unitbytes*i),count,dtype,MPI_ANY_SOURCE,tag,comm,reqs+i);
<a name="line232">232: </a>  }
<a name="line233">233: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) {
<a name="line234">234: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Isend.html#MPI_Isend">MPI_Isend</a>((void*)(tdata+count*unitbytes*i),count,dtype,toranks[i],tag,comm,sendreqs+i);
<a name="line235">235: </a>  }
<a name="line236">236: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</a>(nto+nrecvs,reqs,statuses);
<a name="line237">237: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(nrecvs,&amp;franks);
<a name="line238">238: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nrecvs; i++) franks[i] = statuses[i].MPI_SOURCE;
<a name="line239">239: </a>  <a href="../../../docs/manualpages/Sys/PetscFree2.html#PetscFree2">PetscFree2</a>(reqs,statuses);
<a name="line240">240: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDestroy.html#PetscCommDestroy">PetscCommDestroy</a>(&amp;comm);

<a name="line242">242: </a>  *nfrom            = nrecvs;
<a name="line243">243: </a>  *fromranks        = franks;
<a name="line244">244: </a>  *(void**)fromdata = fdata;
<a name="line245">245: </a>  <font color="#4169E1">return</font>(0);
<a name="line246">246: </a>}
<a name="line247">247: </a><font color="#A020F0">#endif</font>

<a name="line251">251: </a><font color="#B22222">/*@C</font>
<a name="line252">252: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a> - discovers communicating ranks given one-sided information, moving constant-sized data in the process (often message lengths)</font>

<a name="line254">254: </a><font color="#B22222">   Collective on <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a></font>

<a name="line256">256: </a><font color="#B22222">   Input Arguments:</font>
<a name="line257">257: </a><font color="#B22222">+  comm - communicator</font>
<a name="line258">258: </a><font color="#B22222">.  count - number of entries to send/receive (must match on all ranks)</font>
<a name="line259">259: </a><font color="#B22222">.  dtype - datatype to send/receive from each rank (must match on all ranks)</font>
<a name="line260">260: </a><font color="#B22222">.  nto - number of ranks to send data to</font>
<a name="line261">261: </a><font color="#B22222">.  toranks - ranks to send to (array of length nto)</font>
<a name="line262">262: </a><font color="#B22222">-  todata - data to send to each rank (packed)</font>

<a name="line264">264: </a><font color="#B22222">   Output Arguments:</font>
<a name="line265">265: </a><font color="#B22222">+  nfrom - number of ranks receiving messages from</font>
<a name="line266">266: </a><font color="#B22222">.  fromranks - ranks receiving messages from (length nfrom; caller should <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>
<a name="line267">267: </a><font color="#B22222">-  fromdata - packed data from each rank, each with count entries of type dtype (length nfrom, caller responsible for <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>

<a name="line269">269: </a><font color="#B22222">   Level: developer</font>

<a name="line271">271: </a><font color="#B22222">   Options Database Keys:</font>
<a name="line272">272: </a><font color="#B22222">.  -build_twosided &lt;allreduce|ibarrier|redscatter&gt; - algorithm to set up two-sided communication</font>

<a name="line274">274: </a><font color="#B22222">   Notes:</font>
<a name="line275">275: </a><font color="#B22222">   This memory-scalable interface is an alternative to calling <a href="../../../docs/manualpages/Sys/PetscGatherNumberOfMessages.html#PetscGatherNumberOfMessages">PetscGatherNumberOfMessages</a>() and</font>
<a name="line276">276: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscGatherMessageLengths.html#PetscGatherMessageLengths">PetscGatherMessageLengths</a>(), possibly with a subsequent round of communication to send other constant-size data.</font>

<a name="line278">278: </a><font color="#B22222">   Basic data types as well as contiguous types are supported, but non-contiguous (e.g., strided) types are not.</font>

<a name="line280">280: </a><font color="#B22222">   References:</font>
<a name="line281">281: </a><font color="#B22222">.  1. - Hoefler, Siebert and Lumsdaine, The <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html#MPI_Ibarrier">MPI_Ibarrier</a> implementation uses the algorithm in</font>
<a name="line282">282: </a><font color="#B22222">   Scalable communication protocols for dynamic sparse data exchange, 2010.</font>

<a name="line284">284: </a><font color="#B22222">.seealso: <a href="../../../docs/manualpages/Sys/PetscGatherNumberOfMessages.html#PetscGatherNumberOfMessages">PetscGatherNumberOfMessages</a>(), <a href="../../../docs/manualpages/Sys/PetscGatherMessageLengths.html#PetscGatherMessageLengths">PetscGatherMessageLengths</a>()</font>
<a name="line285">285: </a><font color="#B22222">@*/</font>
<a name="line286">286: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSided"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata)</font></strong>
<a name="line287">287: </a>{
<a name="line288">288: </a>  <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a>         ierr;
<a name="line289">289: </a>  <a href="../../../docs/manualpages/Sys/PetscBuildTwoSidedType.html#PetscBuildTwoSidedType">PetscBuildTwoSidedType</a> buildtype = PETSC_BUILDTWOSIDED_NOTSET;

<a name="line292">292: </a>  <a href="../../../docs/manualpages/Viewer/PetscSysInitializePackage.html#PetscSysInitializePackage">PetscSysInitializePackage</a>();
<a name="line293">293: </a>  <a href="../../../docs/manualpages/Profiling/PetscLogEventBegin.html#PetscLogEventBegin">PetscLogEventBegin</a>(PETSC_BuildTwoSided,0,0,0,0);
<a name="line294">294: </a>  <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedGetType.html#PetscCommBuildTwoSidedGetType">PetscCommBuildTwoSidedGetType</a>(comm,&amp;buildtype);
<a name="line295">295: </a>  <font color="#4169E1">switch</font> (buildtype) {
<a name="line296">296: </a>  <font color="#4169E1">case</font> PETSC_BUILDTWOSIDED_IBARRIER:
<a name="line297">297: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_IBARRIER) || defined(PETSC_HAVE_MPIX_IBARRIER)</font>
<a name="line298">298: </a>    PetscCommBuildTwoSided_Ibarrier(comm,count,dtype,nto,toranks,todata,nfrom,fromranks,fromdata);
<a name="line299">299: </a><font color="#A020F0">#else</font>
<a name="line300">300: </a>    <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(comm,PETSC_ERR_PLIB,<font color="#666666">"MPI implementation does not provide <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html#MPI_Ibarrier">MPI_Ibarrier</a> (part of MPI-3)"</font>);
<a name="line301">301: </a><font color="#A020F0">#endif</font>
<a name="line302">302: </a>    <font color="#4169E1">break</font>;
<a name="line303">303: </a>  <font color="#4169E1">case</font> PETSC_BUILDTWOSIDED_ALLREDUCE:
<a name="line304">304: </a>    PetscCommBuildTwoSided_Allreduce(comm,count,dtype,nto,toranks,todata,nfrom,fromranks,fromdata);
<a name="line305">305: </a>    <font color="#4169E1">break</font>;
<a name="line306">306: </a>  <font color="#4169E1">case</font> PETSC_BUILDTWOSIDED_REDSCATTER:
<a name="line307">307: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_REDUCE_SCATTER_BLOCK)</font>
<a name="line308">308: </a>    PetscCommBuildTwoSided_RedScatter(comm,count,dtype,nto,toranks,todata,nfrom,fromranks,fromdata);
<a name="line309">309: </a><font color="#A020F0">#else</font>
<a name="line310">310: </a>    <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(comm,PETSC_ERR_PLIB,<font color="#666666">"MPI implementation does not provide <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Reduce_scatter_block.html#MPI_Reduce_scatter_block">MPI_Reduce_scatter_block</a> (part of MPI-2.2)"</font>);
<a name="line311">311: </a><font color="#A020F0">#endif</font>
<a name="line312">312: </a>    <font color="#4169E1">break</font>;
<a name="line313">313: </a><strong><font color="#FF0000">  default:</font></strong> <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(comm,PETSC_ERR_PLIB,<font color="#666666">"Unknown method for building two-sided communication"</font>);
<a name="line314">314: </a>  }
<a name="line315">315: </a>  <a href="../../../docs/manualpages/Profiling/PetscLogEventEnd.html#PetscLogEventEnd">PetscLogEventEnd</a>(PETSC_BuildTwoSided,0,0,0,0);
<a name="line316">316: </a>  <font color="#4169E1">return</font>(0);
<a name="line317">317: </a>}

<a name="line321">321: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSidedFReq_Reference"></a>static <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> PetscCommBuildTwoSidedFReq_Reference(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,</font></strong>
<a name="line322">322: </a><strong><font color="#4169E1">                                                           <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> ntags,MPI_Request **toreqs,MPI_Request **fromreqs,</font></strong>
<a name="line323">323: </a><strong><font color="#4169E1">                                                           <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*send)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),</font></strong>
<a name="line324">324: </a><strong><font color="#4169E1">                                                           <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*recv)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),void *ctx)</font></strong>
<a name="line325">325: </a>{
<a name="line327">327: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> i,*tag;
<a name="line328">328: </a>  MPI_Aint    lb,unitbytes;
<a name="line329">329: </a>  MPI_Request *sendreq,*recvreq;

<a name="line332">332: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(ntags,&amp;tag);
<a name="line333">333: </a>  <font color="#4169E1">if</font> (ntags &gt; 0) {
<a name="line334">334: </a>    <a href="../../../docs/manualpages/Sys/PetscCommDuplicate.html#PetscCommDuplicate">PetscCommDuplicate</a>(comm,&amp;comm,&amp;tag[0]);
<a name="line335">335: </a>  }
<a name="line336">336: </a>  <font color="#4169E1">for</font> (i=1; i&lt;ntags; i++) {
<a name="line337">337: </a>    <a href="../../../docs/manualpages/Sys/PetscCommGetNewTag.html#PetscCommGetNewTag">PetscCommGetNewTag</a>(comm,&amp;tag[i]);
<a name="line338">338: </a>  }

<a name="line340">340: </a>  <font color="#B22222">/* Perform complete initial rendezvous */</font>
<a name="line341">341: </a>  <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>(comm,count,dtype,nto,toranks,todata,nfrom,fromranks,fromdata);

<a name="line343">343: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(nto*ntags,&amp;sendreq);
<a name="line344">344: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(*nfrom*ntags,&amp;recvreq);

<a name="line346">346: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Type_get_extent.html#MPI_Type_get_extent">MPI_Type_get_extent</a>(dtype,&amp;lb,&amp;unitbytes);
<a name="line347">347: </a>  <font color="#4169E1">if</font> (lb != 0) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(comm,PETSC_ERR_SUP,<font color="#666666">"Datatype with nonzero lower bound %ld\n"</font>,(long)lb);
<a name="line348">348: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) {
<a name="line349">349: </a>    <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> k;
<a name="line350">350: </a>    <font color="#4169E1">for</font> (k=0; k&lt;ntags; k++) sendreq[i*ntags+k] = MPI_REQUEST_NULL;
<a name="line351">351: </a>    (*send)(comm,tag,i,toranks[i],((char*)todata)+count*unitbytes*i,sendreq+i*ntags,ctx);
<a name="line352">352: </a>  }
<a name="line353">353: </a>  <font color="#4169E1">for</font> (i=0; i&lt;*nfrom; i++) {
<a name="line354">354: </a>    void *header = (*(char**)fromdata) + count*unitbytes*i;
<a name="line355">355: </a>    <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> k;
<a name="line356">356: </a>    <font color="#4169E1">for</font> (k=0; k&lt;ntags; k++) recvreq[i*ntags+k] = MPI_REQUEST_NULL;
<a name="line357">357: </a>    (*recv)(comm,tag,(*fromranks)[i],header,recvreq+i*ntags,ctx);
<a name="line358">358: </a>  }
<a name="line359">359: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(tag);
<a name="line360">360: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDestroy.html#PetscCommDestroy">PetscCommDestroy</a>(&amp;comm);
<a name="line361">361: </a>  *toreqs = sendreq;
<a name="line362">362: </a>  *fromreqs = recvreq;
<a name="line363">363: </a>  <font color="#4169E1">return</font>(0);
<a name="line364">364: </a>}

<a name="line366">366: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_IBARRIER) || defined(PETSC_HAVE_MPIX_IBARRIER)</font>

<a name="line370">370: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSidedFReq_Ibarrier"></a>static <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> PetscCommBuildTwoSidedFReq_Ibarrier(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,</font></strong>
<a name="line371">371: </a><strong><font color="#4169E1">                                                          <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> ntags,MPI_Request **toreqs,MPI_Request **fromreqs,</font></strong>
<a name="line372">372: </a><strong><font color="#4169E1">                                                          <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*send)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),</font></strong>
<a name="line373">373: </a><strong><font color="#4169E1">                                                          <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*recv)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),void *ctx)</font></strong>
<a name="line374">374: </a>{
<a name="line376">376: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>    nrecvs,tag,*tags,done,i;
<a name="line377">377: </a>  MPI_Aint       lb,unitbytes;
<a name="line378">378: </a>  char           *tdata;
<a name="line379">379: </a>  MPI_Request    *sendreqs,*usendreqs,*req,barrier;
<a name="line380">380: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBuffer.html#PetscSegBuffer">PetscSegBuffer</a> segrank,segdata,segreq;
<a name="line381">381: </a>  <a href="../../../docs/manualpages/Sys/PetscBool.html#PetscBool">PetscBool</a>      barrier_started;

<a name="line384">384: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDuplicate.html#PetscCommDuplicate">PetscCommDuplicate</a>(comm,&amp;comm,&amp;tag);
<a name="line385">385: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(ntags,&amp;tags);
<a name="line386">386: </a>  <font color="#4169E1">for</font> (i=0; i&lt;ntags; i++) {
<a name="line387">387: </a>    <a href="../../../docs/manualpages/Sys/PetscCommGetNewTag.html#PetscCommGetNewTag">PetscCommGetNewTag</a>(comm,&amp;tags[i]);
<a name="line388">388: </a>  }
<a name="line389">389: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Type_get_extent.html#MPI_Type_get_extent">MPI_Type_get_extent</a>(dtype,&amp;lb,&amp;unitbytes);
<a name="line390">390: </a>  <font color="#4169E1">if</font> (lb != 0) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(comm,PETSC_ERR_SUP,<font color="#666666">"Datatype with nonzero lower bound %ld\n"</font>,(long)lb);
<a name="line391">391: </a>  tdata = (char*)todata;
<a name="line392">392: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(nto,&amp;sendreqs);
<a name="line393">393: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(nto*ntags,&amp;usendreqs);
<a name="line394">394: </a>  <font color="#B22222">/* Post synchronous sends */</font>
<a name="line395">395: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) {
<a name="line396">396: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Issend.html#MPI_Issend">MPI_Issend</a>((void*)(tdata+count*unitbytes*i),count,dtype,toranks[i],tag,comm,sendreqs+i);
<a name="line397">397: </a>  }
<a name="line398">398: </a>  <font color="#B22222">/* Post actual payloads.  These are typically larger messages.  Hopefully sending these later does not slow down the</font>
<a name="line399">399: </a><font color="#B22222">   * synchronous messages above. */</font>
<a name="line400">400: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) {
<a name="line401">401: </a>    <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> k;
<a name="line402">402: </a>    <font color="#4169E1">for</font> (k=0; k&lt;ntags; k++) usendreqs[i*ntags+k] = MPI_REQUEST_NULL;
<a name="line403">403: </a>    (*send)(comm,tags,i,toranks[i],tdata+count*unitbytes*i,usendreqs+i*ntags,ctx);
<a name="line404">404: </a>  }

<a name="line406">406: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferCreate.html#PetscSegBufferCreate">PetscSegBufferCreate</a>(<font color="#4169E1">sizeof</font>(<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>),4,&amp;segrank);
<a name="line407">407: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferCreate.html#PetscSegBufferCreate">PetscSegBufferCreate</a>(unitbytes,4*count,&amp;segdata);
<a name="line408">408: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferCreate.html#PetscSegBufferCreate">PetscSegBufferCreate</a>(<font color="#4169E1">sizeof</font>(MPI_Request),4,&amp;segreq);

<a name="line410">410: </a>  nrecvs  = 0;
<a name="line411">411: </a>  barrier = MPI_REQUEST_NULL;
<a name="line412">412: </a>  <font color="#B22222">/* MPICH-3.2 sometimes does not create a request in some "optimized" cases.  This is arguably a standard violation,</font>
<a name="line413">413: </a><font color="#B22222">   * but we need to work around it. */</font>
<a name="line414">414: </a>  barrier_started = <a href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</a>;
<a name="line415">415: </a>  <font color="#4169E1">for</font> (done=0; !done; ) {
<a name="line416">416: </a>    <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> flag;
<a name="line417">417: </a>    MPI_Status  status;
<a name="line418">418: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Iprobe.html#MPI_Iprobe">MPI_Iprobe</a>(MPI_ANY_SOURCE,tag,comm,&amp;flag,&amp;status);
<a name="line419">419: </a>    <font color="#4169E1">if</font> (flag) {                 <font color="#B22222">/* incoming message */</font>
<a name="line420">420: </a>      <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *recvrank,k;
<a name="line421">421: </a>      void        *buf;
<a name="line422">422: </a>      <a href="../../../docs/manualpages/Sys/PetscSegBufferGet.html#PetscSegBufferGet">PetscSegBufferGet</a>(segrank,1,&amp;recvrank);
<a name="line423">423: </a>      <a href="../../../docs/manualpages/Sys/PetscSegBufferGet.html#PetscSegBufferGet">PetscSegBufferGet</a>(segdata,count,&amp;buf);
<a name="line424">424: </a>      *recvrank = status.MPI_SOURCE;
<a name="line425">425: </a>      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Recv.html#MPI_Recv">MPI_Recv</a>(buf,count,dtype,status.MPI_SOURCE,tag,comm,MPI_STATUS_IGNORE);
<a name="line426">426: </a>      <a href="../../../docs/manualpages/Sys/PetscSegBufferGet.html#PetscSegBufferGet">PetscSegBufferGet</a>(segreq,ntags,&amp;req);
<a name="line427">427: </a>      <font color="#4169E1">for</font> (k=0; k&lt;ntags; k++) req[k] = MPI_REQUEST_NULL;
<a name="line428">428: </a>      (*recv)(comm,tags,status.MPI_SOURCE,buf,req,ctx);
<a name="line429">429: </a>      nrecvs++;
<a name="line430">430: </a>    }
<a name="line431">431: </a>    <font color="#4169E1">if</font> (!barrier_started) {
<a name="line432">432: </a>      <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> sent,nsends;
<a name="line433">433: </a>      <a href="../../../docs/manualpages/Sys/PetscMPIIntCast.html#PetscMPIIntCast">PetscMPIIntCast</a>(nto,&amp;nsends);
<a name="line434">434: </a>      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Testall.html#MPI_Testall">MPI_Testall</a>(nsends,sendreqs,&amp;sent,MPI_STATUSES_IGNORE);
<a name="line435">435: </a>      <font color="#4169E1">if</font> (sent) {
<a name="line436">436: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_IBARRIER)</font>
<a name="line437">437: </a>        <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html#MPI_Ibarrier">MPI_Ibarrier</a>(comm,&amp;barrier);
<a name="line438">438: </a><font color="#A020F0">#elif defined(PETSC_HAVE_MPIX_IBARRIER)</font>
<a name="line439">439: </a>        MPIX_Ibarrier(comm,&amp;barrier);
<a name="line440">440: </a><font color="#A020F0">#endif</font>
<a name="line441">441: </a>        barrier_started = <a href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</a>;
<a name="line442">442: </a>      }
<a name="line443">443: </a>    } <font color="#4169E1">else</font> {
<a name="line444">444: </a>      <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Test.html#MPI_Test">MPI_Test</a>(&amp;barrier,&amp;done,MPI_STATUS_IGNORE);
<a name="line445">445: </a>    }
<a name="line446">446: </a>  }
<a name="line447">447: </a>  *nfrom = nrecvs;
<a name="line448">448: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferExtractAlloc.html#PetscSegBufferExtractAlloc">PetscSegBufferExtractAlloc</a>(segrank,fromranks);
<a name="line449">449: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferDestroy.html#PetscSegBufferDestroy">PetscSegBufferDestroy</a>(&amp;segrank);
<a name="line450">450: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferExtractAlloc.html#PetscSegBufferExtractAlloc">PetscSegBufferExtractAlloc</a>(segdata,fromdata);
<a name="line451">451: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferDestroy.html#PetscSegBufferDestroy">PetscSegBufferDestroy</a>(&amp;segdata);
<a name="line452">452: </a>  *toreqs = usendreqs;
<a name="line453">453: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferExtractAlloc.html#PetscSegBufferExtractAlloc">PetscSegBufferExtractAlloc</a>(segreq,fromreqs);
<a name="line454">454: </a>  <a href="../../../docs/manualpages/Sys/PetscSegBufferDestroy.html#PetscSegBufferDestroy">PetscSegBufferDestroy</a>(&amp;segreq);
<a name="line455">455: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(sendreqs);
<a name="line456">456: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(tags);
<a name="line457">457: </a>  <a href="../../../docs/manualpages/Sys/PetscCommDestroy.html#PetscCommDestroy">PetscCommDestroy</a>(&amp;comm);
<a name="line458">458: </a>  <font color="#4169E1">return</font>(0);
<a name="line459">459: </a>}
<a name="line460">460: </a><font color="#A020F0">#endif</font>

<a name="line464">464: </a><font color="#B22222">/*@C</font>
<a name="line465">465: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedF.html#PetscCommBuildTwoSidedF">PetscCommBuildTwoSidedF</a> - discovers communicating ranks given one-sided information, calling user-defined functions during rendezvous</font>

<a name="line467">467: </a><font color="#B22222">   Collective on <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a></font>

<a name="line469">469: </a><font color="#B22222">   Input Arguments:</font>
<a name="line470">470: </a><font color="#B22222">+  comm - communicator</font>
<a name="line471">471: </a><font color="#B22222">.  count - number of entries to send/receive in initial rendezvous (must match on all ranks)</font>
<a name="line472">472: </a><font color="#B22222">.  dtype - datatype to send/receive from each rank (must match on all ranks)</font>
<a name="line473">473: </a><font color="#B22222">.  nto - number of ranks to send data to</font>
<a name="line474">474: </a><font color="#B22222">.  toranks - ranks to send to (array of length nto)</font>
<a name="line475">475: </a><font color="#B22222">.  todata - data to send to each rank (packed)</font>
<a name="line476">476: </a><font color="#B22222">.  ntags - number of tags needed by send/recv callbacks</font>
<a name="line477">477: </a><font color="#B22222">.  send - callback invoked on sending process when ready to send primary payload</font>
<a name="line478">478: </a><font color="#B22222">.  recv - callback invoked on receiving process after delivery of rendezvous message</font>
<a name="line479">479: </a><font color="#B22222">-  ctx - context for callbacks</font>

<a name="line481">481: </a><font color="#B22222">   Output Arguments:</font>
<a name="line482">482: </a><font color="#B22222">+  nfrom - number of ranks receiving messages from</font>
<a name="line483">483: </a><font color="#B22222">.  fromranks - ranks receiving messages from (length nfrom; caller should <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>
<a name="line484">484: </a><font color="#B22222">-  fromdata - packed data from each rank, each with count entries of type dtype (length nfrom, caller responsible for <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>

<a name="line486">486: </a><font color="#B22222">   Level: developer</font>

<a name="line488">488: </a><font color="#B22222">   Notes:</font>
<a name="line489">489: </a><font color="#B22222">   This memory-scalable interface is an alternative to calling <a href="../../../docs/manualpages/Sys/PetscGatherNumberOfMessages.html#PetscGatherNumberOfMessages">PetscGatherNumberOfMessages</a>() and</font>
<a name="line490">490: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscGatherMessageLengths.html#PetscGatherMessageLengths">PetscGatherMessageLengths</a>(), possibly with a subsequent round of communication to send other data.</font>

<a name="line492">492: </a><font color="#B22222">   Basic data types as well as contiguous types are supported, but non-contiguous (e.g., strided) types are not.</font>

<a name="line494">494: </a><font color="#B22222">   References:</font>
<a name="line495">495: </a><font color="#B22222">.  1. - Hoefler, Siebert and Lumsdaine, The <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html#MPI_Ibarrier">MPI_Ibarrier</a> implementation uses the algorithm in</font>
<a name="line496">496: </a><font color="#B22222">   Scalable communication protocols for dynamic sparse data exchange, 2010.</font>

<a name="line498">498: </a><font color="#B22222">.seealso: <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>(), <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedFReq.html#PetscCommBuildTwoSidedFReq">PetscCommBuildTwoSidedFReq</a>(), <a href="../../../docs/manualpages/Sys/PetscGatherNumberOfMessages.html#PetscGatherNumberOfMessages">PetscGatherNumberOfMessages</a>(), <a href="../../../docs/manualpages/Sys/PetscGatherMessageLengths.html#PetscGatherMessageLengths">PetscGatherMessageLengths</a>()</font>
<a name="line499">499: </a><font color="#B22222">@*/</font>
<a name="line500">500: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSidedF"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedF.html#PetscCommBuildTwoSidedF">PetscCommBuildTwoSidedF</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> ntags,</font></strong>
<a name="line501">501: </a><strong><font color="#4169E1">                                       <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*send)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),</font></strong>
<a name="line502">502: </a><strong><font color="#4169E1">                                       <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*recv)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),void *ctx)</font></strong>
<a name="line503">503: </a>{
<a name="line505">505: </a>  MPI_Request    *toreqs,*fromreqs;

<a name="line508">508: </a>  <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedFReq.html#PetscCommBuildTwoSidedFReq">PetscCommBuildTwoSidedFReq</a>(comm,count,dtype,nto,toranks,todata,nfrom,fromranks,fromdata,ntags,&amp;toreqs,&amp;fromreqs,send,recv,ctx);
<a name="line509">509: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</a>(nto*ntags,toreqs,MPI_STATUSES_IGNORE);
<a name="line510">510: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Waitall.html#MPI_Waitall">MPI_Waitall</a>(*nfrom*ntags,fromreqs,MPI_STATUSES_IGNORE);
<a name="line511">511: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(toreqs);
<a name="line512">512: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(fromreqs);
<a name="line513">513: </a>  <font color="#4169E1">return</font>(0);
<a name="line514">514: </a>}

<a name="line518">518: </a><font color="#B22222">/*@C</font>
<a name="line519">519: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedFReq.html#PetscCommBuildTwoSidedFReq">PetscCommBuildTwoSidedFReq</a> - discovers communicating ranks given one-sided information, calling user-defined functions during rendezvous, returns requests</font>

<a name="line521">521: </a><font color="#B22222">   Collective on <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a></font>

<a name="line523">523: </a><font color="#B22222">   Input Arguments:</font>
<a name="line524">524: </a><font color="#B22222">+  comm - communicator</font>
<a name="line525">525: </a><font color="#B22222">.  count - number of entries to send/receive in initial rendezvous (must match on all ranks)</font>
<a name="line526">526: </a><font color="#B22222">.  dtype - datatype to send/receive from each rank (must match on all ranks)</font>
<a name="line527">527: </a><font color="#B22222">.  nto - number of ranks to send data to</font>
<a name="line528">528: </a><font color="#B22222">.  toranks - ranks to send to (array of length nto)</font>
<a name="line529">529: </a><font color="#B22222">.  todata - data to send to each rank (packed)</font>
<a name="line530">530: </a><font color="#B22222">.  ntags - number of tags needed by send/recv callbacks</font>
<a name="line531">531: </a><font color="#B22222">.  send - callback invoked on sending process when ready to send primary payload</font>
<a name="line532">532: </a><font color="#B22222">.  recv - callback invoked on receiving process after delivery of rendezvous message</font>
<a name="line533">533: </a><font color="#B22222">-  ctx - context for callbacks</font>

<a name="line535">535: </a><font color="#B22222">   Output Arguments:</font>
<a name="line536">536: </a><font color="#B22222">+  nfrom - number of ranks receiving messages from</font>
<a name="line537">537: </a><font color="#B22222">.  fromranks - ranks receiving messages from (length nfrom; caller should <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>
<a name="line538">538: </a><font color="#B22222">.  fromdata - packed data from each rank, each with count entries of type dtype (length nfrom, caller responsible for <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>
<a name="line539">539: </a><font color="#B22222">.  toreqs - array of nto*ntags sender requests (caller must wait on these, then <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>
<a name="line540">540: </a><font color="#B22222">-  fromreqs - array of nfrom*ntags receiver requests (caller must wait on these, then <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>())</font>

<a name="line542">542: </a><font color="#B22222">   Level: developer</font>

<a name="line544">544: </a><font color="#B22222">   Notes:</font>
<a name="line545">545: </a><font color="#B22222">   This memory-scalable interface is an alternative to calling <a href="../../../docs/manualpages/Sys/PetscGatherNumberOfMessages.html#PetscGatherNumberOfMessages">PetscGatherNumberOfMessages</a>() and</font>
<a name="line546">546: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscGatherMessageLengths.html#PetscGatherMessageLengths">PetscGatherMessageLengths</a>(), possibly with a subsequent round of communication to send other data.</font>

<a name="line548">548: </a><font color="#B22222">   Basic data types as well as contiguous types are supported, but non-contiguous (e.g., strided) types are not.</font>

<a name="line550">550: </a><font color="#B22222">   References:</font>
<a name="line551">551: </a><font color="#B22222">.  1. - Hoefler, Siebert and Lumsdaine, The <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html#MPI_Ibarrier">MPI_Ibarrier</a> implementation uses the algorithm in</font>
<a name="line552">552: </a><font color="#B22222">   Scalable communication protocols for dynamic sparse data exchange, 2010.</font>

<a name="line554">554: </a><font color="#B22222">.seealso: <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSided.html#PetscCommBuildTwoSided">PetscCommBuildTwoSided</a>(), <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedF.html#PetscCommBuildTwoSidedF">PetscCommBuildTwoSidedF</a>(), <a href="../../../docs/manualpages/Sys/PetscGatherNumberOfMessages.html#PetscGatherNumberOfMessages">PetscGatherNumberOfMessages</a>(), <a href="../../../docs/manualpages/Sys/PetscGatherMessageLengths.html#PetscGatherMessageLengths">PetscGatherMessageLengths</a>()</font>
<a name="line555">555: </a><font color="#B22222">@*/</font>
<a name="line556">556: </a><strong><font color="#4169E1"><a name="PetscCommBuildTwoSidedFReq"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedFReq.html#PetscCommBuildTwoSidedFReq">PetscCommBuildTwoSidedFReq</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> count,MPI_Datatype dtype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> nto,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *toranks,const void *todata,</font></strong>
<a name="line557">557: </a><strong><font color="#4169E1">                                          <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *nfrom,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> **fromranks,void *fromdata,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> ntags,MPI_Request **toreqs,MPI_Request **fromreqs,</font></strong>
<a name="line558">558: </a><strong><font color="#4169E1">                                          <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*send)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),</font></strong>
<a name="line559">559: </a><strong><font color="#4169E1">                                          <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*recv)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),void *ctx)</font></strong>
<a name="line560">560: </a>{
<a name="line561">561: </a>  <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a>         ierr,(*f)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,MPI_Datatype,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],const void*,
<a name="line562">562: </a>                                   <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>*,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>**,void*,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,MPI_Request**,MPI_Request**,
<a name="line563">563: </a>                                   <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*send)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),
<a name="line564">564: </a>                                   <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> (*recv)(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>,const <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>[],<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>,void*,MPI_Request[],void*),void *ctx);
<a name="line565">565: </a>  <a href="../../../docs/manualpages/Sys/PetscBuildTwoSidedType.html#PetscBuildTwoSidedType">PetscBuildTwoSidedType</a> buildtype = PETSC_BUILDTWOSIDED_NOTSET;
<a name="line566">566: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> i,size;

<a name="line569">569: </a>  <a href="../../../docs/manualpages/Viewer/PetscSysInitializePackage.html#PetscSysInitializePackage">PetscSysInitializePackage</a>();
<a name="line570">570: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>(comm,&amp;size);
<a name="line571">571: </a>  <font color="#4169E1">for</font> (i=0; i&lt;nto; i++) {
<a name="line572">572: </a>    <font color="#4169E1">if</font> (toranks[i] &lt; 0 || size &lt;= toranks[i]) <a href="../../../docs/manualpages/Sys/SETERRQ3.html#SETERRQ3">SETERRQ3</a>(comm,PETSC_ERR_ARG_OUTOFRANGE,<font color="#666666">"toranks[%d] %d not in comm size %d"</font>,i,toranks[i],size);
<a name="line573">573: </a>  }
<a name="line574">574: </a>  <a href="../../../docs/manualpages/Profiling/PetscLogEventBegin.html#PetscLogEventBegin">PetscLogEventBegin</a>(PETSC_BuildTwoSidedF,0,0,0,0);
<a name="line575">575: </a>  <a href="../../../docs/manualpages/Sys/PetscCommBuildTwoSidedGetType.html#PetscCommBuildTwoSidedGetType">PetscCommBuildTwoSidedGetType</a>(comm,&amp;buildtype);
<a name="line576">576: </a>  <font color="#4169E1">switch</font> (buildtype) {
<a name="line577">577: </a>  <font color="#4169E1">case</font> PETSC_BUILDTWOSIDED_IBARRIER:
<a name="line578">578: </a><font color="#A020F0">#if defined(PETSC_HAVE_MPI_IBARRIER) || defined(PETSC_HAVE_MPIX_IBARRIER)</font>
<a name="line579">579: </a>    f = PetscCommBuildTwoSidedFReq_Ibarrier;
<a name="line580">580: </a><font color="#A020F0">#else</font>
<a name="line581">581: </a>    <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(comm,PETSC_ERR_PLIB,<font color="#666666">"MPI implementation does not provide <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Ibarrier.html#MPI_Ibarrier">MPI_Ibarrier</a> (part of MPI-3)"</font>);
<a name="line582">582: </a><font color="#A020F0">#endif</font>
<a name="line583">583: </a>    <font color="#4169E1">break</font>;
<a name="line584">584: </a>  <font color="#4169E1">case</font> PETSC_BUILDTWOSIDED_ALLREDUCE:
<a name="line585">585: </a>  <font color="#4169E1">case</font> PETSC_BUILDTWOSIDED_REDSCATTER:
<a name="line586">586: </a>    f = PetscCommBuildTwoSidedFReq_Reference;
<a name="line587">587: </a>    <font color="#4169E1">break</font>;
<a name="line588">588: </a><strong><font color="#FF0000">  default:</font></strong> <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(comm,PETSC_ERR_PLIB,<font color="#666666">"Unknown method for building two-sided communication"</font>);
<a name="line589">589: </a>  }
<a name="line590">590: </a>  (*f)(comm,count,dtype,nto,toranks,todata,nfrom,fromranks,fromdata,ntags,toreqs,fromreqs,send,recv,ctx);
<a name="line591">591: </a>  <a href="../../../docs/manualpages/Profiling/PetscLogEventEnd.html#PetscLogEventEnd">PetscLogEventEnd</a>(PETSC_BuildTwoSidedF,0,0,0,0);
<a name="line592">592: </a>  <font color="#4169E1">return</font>(0);
<a name="line593">593: </a>}
</pre>
</body>

</html>
